1. UTF-32 must be nice for any operation requiring the non-English alphabet - e.g. when typing with emojis - because all of its encoding happens in one number. However, like the question mentioned, it's also not efficient for ASCII characters, since 3 bytes are dedicated to holding nothing but ones. UTF-8 is the opposite: it's nice for ASCII because there are no wasted bytes, but anything requiring more than one byte takes extra work to handle.
2. The 10 is useful because it specifies with no doubt that those bytes are dedicated to continuing the byte called 1110XXXX. It could be nice to save the bits, but the loss of space isn't as bad as not having a definitive method of knowing where the bytes of a codepoint ends. Without the continuing byte notation, it'd take more effort to make sure there are no bugs in multilingual programs or programs that use emojis.